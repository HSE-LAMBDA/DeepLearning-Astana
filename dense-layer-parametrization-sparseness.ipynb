{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%env THEANO_FLAGS='device=gpu0','floatX=float32'\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne import *\n",
    "\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mldm import NNWatcher, Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "wget -q -nc https://raw.githubusercontent.com/amitgroup/amitgroup/master/amitgroup/io/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### http://g.sweyla.com/blog/2012/mnist-numpy/\n",
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir -p mnist && {\n",
    "    cd mnist;\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz &&\n",
    "    gunzip *.gz\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = mnist.load_mnist(dataset='training', path='mnist/')\n",
    "X = X.reshape(-1, 1, 28, 28).astype('float32')\n",
    "\n",
    "X_test, y_test = mnist.load_mnist(dataset='testing', path='mnist/')\n",
    "X_test = X_test.reshape(-1, 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y, n_classes=10):\n",
    "    onehot = np.zeros(shape=(y.shape[0], n_classes), dtype='float32')\n",
    "\n",
    "    onehot[np.arange(y.shape[0]), y] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = one_hot(y)\n",
    "y_test = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.prod(X.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deep\n",
    "but this time not exponentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparseness(W, c):\n",
    "    n_units = W.get_value().shape[1]\n",
    "    \n",
    "    l1_units = T.sum(abs(W), axis=1)\n",
    "    l2_units = T.sqrt(T.sum(abs(W), axis=1))\n",
    "    \n",
    "    sp = (np.sqrt(n_units) - l1_units / l2_units) / (np.sqrt(n_units) - 1)\n",
    "    \n",
    "    constraints = T.nnet.softplus(c - sp)\n",
    "    penalty = T.sum(constraints)\n",
    "    return penalty\n",
    "\n",
    "class SparseLayer(layers.Layer):\n",
    "    def __init__(self, incoming, num_units,\n",
    "                 W=init.GlorotUniform(),\n",
    "                 b=init.Constant(0.),\n",
    "                 nonlinearity=nonlinearities.rectify,\n",
    "                 **kwargs):\n",
    "        super(SparseLayer, self).__init__(incoming, **kwargs)\n",
    "\n",
    "        num_inputs = self.input_shape[1]\n",
    "        self.num_units = num_units\n",
    "\n",
    "        self.W = self.add_param(W, (num_inputs, num_units), name='W')\n",
    "        self.b = self.add_param(b, (num_units, ), name='b', regularizable=False)\n",
    "        \n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        activation = T.dot(input, self.W) + self.b\n",
    "\n",
    "        return self.nonlinearity(activation)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape[:1] + (self.num_units,)\n",
    "    \n",
    "    def get_sparseness_penalty(self, sparsness_c=0.85):\n",
    "        return sparseness(self.W, sparsness_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DeepSparseMNISTNet(Net):\n",
    "    def __init__(self, sparseness_c=0.85):\n",
    "        self.X_batch = T.ftensor4(name='X_batch')\n",
    "        self.y_batch = T.fmatrix(name='y_batch')\n",
    "        \n",
    "        self.layers = []\n",
    "        input_l = layers.InputLayer(shape=(None, ) + X.shape[1:], input_var=self.X_batch, name='Input')\n",
    "        self.layers.append(input_l)\n",
    "        \n",
    "        reshape_l = layers.FlattenLayer(input_l)\n",
    "        self.layers.append(reshape_l)\n",
    "\n",
    "        for i, n_units in enumerate([128, 64, 16]):\n",
    "            dense = SparseLayer(\n",
    "                self.layers[-1],\n",
    "                num_units=n_units,\n",
    "                nonlinearity=nonlinearities.sigmoid,\n",
    "                name='SparseLayer %d' % i\n",
    "            )\n",
    "\n",
    "            self.layers.append(dense)\n",
    "        \"\"\"\n",
    "        self.layers.append(\n",
    "            layers.DenseLayer(\n",
    "                self.layers[-1],\n",
    "                num_units = 16,\n",
    "                nonlinearity=nonlinearities.sigmoid,\n",
    "                name = 'DenseLayer 1'\n",
    "            )\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        output_l = layers.DenseLayer(\n",
    "            self.layers[-1],\n",
    "            num_units=10,\n",
    "            nonlinearity=nonlinearities.softmax,\n",
    "            name= 'Softmax'\n",
    "        )\n",
    "        \n",
    "        self.layers.append(output_l)\n",
    "\n",
    "        self.net = output_l\n",
    "        \n",
    "        self.predictions = layers.get_output(self.net)\n",
    "        self.pure_loss = T.mean(objectives.categorical_crossentropy(self.predictions, self.y_batch))\n",
    "        \n",
    "        self.sparsness_penalty_coef = T.fscalar('sparseness_penalty_coef')\n",
    "        self.regularization_coef = T.fscalar('regularization_coef')\n",
    "        \n",
    "        self.regularization = self.sparsness_penalty_coef * reduce(lambda a, b: a + b, [\n",
    "                layer.get_sparseness_penalty(sparseness_c)\n",
    "                for layer in self.layers\n",
    "                if hasattr(layer, 'get_sparseness_penalty')\n",
    "        ]) + self.regularization_coef * regularization.regularize_network_params(\n",
    "            self.net,\n",
    "            penalty=regularization.l2\n",
    "        )\n",
    "        \n",
    "        self.loss = self.pure_loss + self.regularization\n",
    "        \n",
    "        self.learning_rate = T.fscalar('learning rate')\n",
    "        params = layers.get_all_params(self.net)\n",
    "\n",
    "        upd = updates.adadelta(self.loss, params, learning_rate=self.learning_rate)\n",
    "\n",
    "        self.train = theano.function(\n",
    "            [\n",
    "                self.X_batch, self.y_batch,\n",
    "                self.regularization_coef, self.sparsness_penalty_coef,\n",
    "                self.learning_rate\n",
    "            ],\n",
    "            [self.pure_loss, self.regularization],\n",
    "            updates=upd\n",
    "        )\n",
    "\n",
    "        self.get_loss = theano.function([self.X_batch, self.y_batch], self.pure_loss)\n",
    "        \n",
    "        super(DeepSparseMNISTNet, self).__init__()\n",
    "            \n",
    "    @staticmethod\n",
    "    def batch_stream(n, batch_size=32):\n",
    "        n_batches = n / batch_size\n",
    "        \n",
    "        for i in xrange(n_batches):\n",
    "            indx = np.random.choice(n, size=batch_size)\n",
    "            yield indx\n",
    "    \n",
    "    def fit(self, X, y, n_epoches = 1, batch_size=32,\n",
    "            regularization_coef=1.0e-3, sparsness_penalty_coef=1.0e-3,\n",
    "            learning_rate = 1.0):\n",
    "        regularization_coef = np.float32(regularization_coef)\n",
    "        learning_rate = np.float32(learning_rate)\n",
    "        sparsness_penalty_coef = np.float32(sparsness_penalty_coef)\n",
    "        \n",
    "        n_batches = X.shape[0] / batch_size\n",
    "        losses = np.zeros(shape=(n_epoches, n_batches), dtype='float32')\n",
    "        regs = np.zeros(shape=(n_epoches, n_batches), dtype='float32')\n",
    "        \n",
    "        for epoch in xrange(n_epoches):\n",
    "            for i, indx in enumerate(self.batch_stream(X.shape[0], batch_size=batch_size)):\n",
    "                losses[epoch, i], regs[epoch, i] = \\\n",
    "                self.train(X[indx], y[indx], regularization_coef, sparsness_penalty_coef, learning_rate)\n",
    "            \n",
    "            yield losses[:(epoch + 1)], regs[:(epoch + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deep_net = DeepSparseMNISTNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deep_net.save('deep-sparse-net-0.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watcher = NNWatcher(labels=('loss', 'sparseness penalty'), colors=('blue', 'red'))\n",
    "\n",
    "for loss, reg in deep_net.fit(\n",
    "    X, y, n_epoches=64, batch_size=64,\n",
    "    learning_rate=1.0, \n",
    "    regularization_coef=1.0e-5, sparsness_penalty_coef=1.0e-5\n",
    "):\n",
    "    watcher.draw(loss, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = theano.function([deep_net.X_batch], deep_net.predictions)\n",
    "\n",
    "y_proba = predict(X_test)\n",
    "print 'accuracy:', np.mean(np.argmax(y_test, axis=1) == np.argmax(y_proba, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watcher = NNWatcher(labels=('loss', 'sparseness penalty'), colors=('blue', 'red'))\n",
    "\n",
    "for loss, reg in deep_net.fit(X, y, n_epoches=64, learning_rate=1.0e-1, batch_size=64, regularization_coef=1.0e-3):\n",
    "    watcher.draw(loss, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watcher = NNWatcher(labels=('loss', 'sparseness penalty'), colors=('blue', 'red'))\n",
    "\n",
    "for loss, reg in deep_net.fit(X, y, n_epoches=64, learning_rate=1.0, batch_size=128, regularization_coef=2.5e-3):\n",
    "    watcher.draw(loss, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_net.save('deep-low-parameter-net-1.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = theano.function([deep_net.X_batch], deep_net.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_proba = predict(X_test)\n",
    "print 'accuracy:', np.mean(np.argmax(y_test, axis=1) == np.argmax(y_proba, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = deep_net.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = l.get_params()[0].get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(W, interpolation='None', aspect = 0.1, cmap=plt.cm.viridis)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "n = 5\n",
    "fig, ax = plt.subplots(n, n, figsize=(12, 12))\n",
    "\n",
    "plt.suptitle('Filters of the first dense layer')\n",
    "\n",
    "vmin = np.min(np.abs(W))\n",
    "vmax = np.max(np.abs(W))\n",
    "\n",
    "for i, j in itertools.product(range(n), range(n)):\n",
    "    k = i * n + j\n",
    "    ax[i, j].imshow(\n",
    "        W[:, k].reshape(28, 28), interpolation='None',\n",
    "        cmap=plt.cm.viridis, vmin=vmin, vmax=vmax\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
